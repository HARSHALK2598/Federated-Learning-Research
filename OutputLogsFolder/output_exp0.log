nohup: ignoring input
[2024-12-06 11:03:03,915] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-12-06 11:03:05,044] torch.distributed.run: [WARNING] 
[2024-12-06 11:03:05,044] torch.distributed.run: [WARNING] *****************************************
[2024-12-06 11:03:05,044] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-12-06 11:03:05,044] torch.distributed.run: [WARNING] *****************************************
[2024-12-06 11:03:08,256] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-12-06 11:03:08,261] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-12-06 11:03:08,262] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-12-06 11:03:08,263] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/hsk8171/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/hsk8171/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.Warning: The cache directory for DeepSpeed Triton autotune, /home/hsk8171/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.

Warning: The cache directory for DeepSpeed Triton autotune, /home/hsk8171/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2024-12-06 11:03:09,411] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-12-06 11:03:09,411] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-12-06 11:03:09,411] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-12-06 11:03:09,412] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-12-06 11:03:09,412] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
/ext3/miniforge3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/ext3/miniforge3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
/ext3/miniforge3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/ext3/miniforge3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/ext3/miniforge3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
/ext3/miniforge3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
/ext3/miniforge3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/ext3/miniforge3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Files already downloaded and verified
Files already downloaded and verifiedFiles already downloaded and verifiedFiles already downloaded and verified


Files already downloaded and verified
Files already downloaded and verifiedFiles already downloaded and verifiedFiles already downloaded and verified


[2024-12-06 11:03:11,639] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.0, git-hash=unknown, git-branch=unknown
[2024-12-06 11:03:11,640] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-12-06 11:03:11,786] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-12-06 11:03:11,787] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-12-06 11:03:11,788] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-12-06 11:03:12,202] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-12-06 11:03:12,203] [INFO] [logging.py:128:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-12-06 11:03:12,203] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-12-06 11:03:12,204] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = SGD
[2024-12-06 11:03:12,204] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = SGD
[2024-12-06 11:03:12,204] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2024-12-06 11:03:12,204] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-12-06 11:03:12,204] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0.1], mom=[0.9]
[2024-12-06 11:03:12,204] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-12-06 11:03:12,205] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-12-06 11:03:12,205] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-12-06 11:03:12,205] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-12-06 11:03:12,205] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-12-06 11:03:12,205] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-12-06 11:03:12,205] [INFO] [config.py:1003:print]   bfloat16_enabled ............. False
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x14e2246d9280>
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   gradient_clipping ............ 0.0
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 65536
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   loss_scale ................... 0
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-12-06 11:03:12,206] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-12-06 11:03:12,207] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-12-06 11:03:12,207] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-12-06 11:03:12,207] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-12-06 11:03:12,207] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-12-06 11:03:12,207] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-12-06 11:03:12,207] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-12-06 11:03:12,207] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-12-06 11:03:12,207] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-12-06 11:03:12,207] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-12-06 11:03:12,207] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-12-06 11:03:12,207] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-12-06 11:03:12,207] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-12-06 11:03:12,207] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-12-06 11:03:12,207] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-12-06 11:03:12,207] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-12-06 11:03:12,207] [INFO] [config.py:1003:print]   steps_per_print .............. inf
[2024-12-06 11:03:12,207] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-12-06 11:03:12,207] [INFO] [config.py:1003:print]   train_batch_size ............. 64
[2024-12-06 11:03:12,207] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-12-06 11:03:12,207] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-12-06 11:03:12,207] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-12-06 11:03:12,207] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-12-06 11:03:12,207] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-12-06 11:03:12,207] [INFO] [config.py:1003:print]   world_size ................... 4
[2024-12-06 11:03:12,207] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  True
[2024-12-06 11:03:12,207] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-12-06 11:03:12,207] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-12-06 11:03:12,207] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-12-06 11:03:12,207] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-12-06 11:03:12,207] [INFO] [config.py:989:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 16, 
    "gradient_accumulation_steps": 1, 
    "fp16": {
        "enabled": false
    }, 
    "bf16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 0
    }, 
    "steps_per_print": inf, 
    "zero_allow_untested_optimizer": true
}
Epoch 1/50: Train Loss = 2.2017, Train Accuracy = 28.95%, Test Accuracy = 42.89%, Epoch Time = 8.24s, Total Time = 8.24s
Epoch 2/50: Train Loss = 1.5693, Train Accuracy = 42.80%, Test Accuracy = 49.45%, Epoch Time = 7.67s, Total Time = 15.90s
Epoch 3/50: Train Loss = 1.3772, Train Accuracy = 49.98%, Test Accuracy = 53.87%, Epoch Time = 7.69s, Total Time = 23.59s
Epoch 4/50: Train Loss = 1.2374, Train Accuracy = 55.75%, Test Accuracy = 57.15%, Epoch Time = 7.67s, Total Time = 31.26s
Epoch 5/50: Train Loss = 1.1157, Train Accuracy = 60.47%, Test Accuracy = 60.90%, Epoch Time = 7.68s, Total Time = 38.94s
Epoch 6/50: Train Loss = 1.0137, Train Accuracy = 64.12%, Test Accuracy = 64.45%, Epoch Time = 7.68s, Total Time = 46.62s
Epoch 7/50: Train Loss = 0.9496, Train Accuracy = 66.47%, Test Accuracy = 66.68%, Epoch Time = 7.72s, Total Time = 54.33s
Epoch 8/50: Train Loss = 0.9144, Train Accuracy = 67.97%, Test Accuracy = 64.92%, Epoch Time = 7.72s, Total Time = 62.05s
Epoch 9/50: Train Loss = 0.8564, Train Accuracy = 70.11%, Test Accuracy = 70.94%, Epoch Time = 7.70s, Total Time = 69.75s
Epoch 10/50: Train Loss = 0.8330, Train Accuracy = 70.59%, Test Accuracy = 69.30%, Epoch Time = 7.71s, Total Time = 77.47s
Epoch 11/50: Train Loss = 0.8038, Train Accuracy = 72.23%, Test Accuracy = 68.52%, Epoch Time = 7.74s, Total Time = 85.20s
Epoch 12/50: Train Loss = 0.7625, Train Accuracy = 73.88%, Test Accuracy = 71.84%, Epoch Time = 7.65s, Total Time = 92.85s
Epoch 13/50: Train Loss = 0.7328, Train Accuracy = 74.67%, Test Accuracy = 74.57%, Epoch Time = 7.84s, Total Time = 100.69s
Epoch 14/50: Train Loss = 0.7296, Train Accuracy = 74.77%, Test Accuracy = 68.05%, Epoch Time = 7.88s, Total Time = 108.57s
Epoch 15/50: Train Loss = 0.7095, Train Accuracy = 75.62%, Test Accuracy = 74.45%, Epoch Time = 7.71s, Total Time = 116.29s
Epoch 16/50: Train Loss = 0.7000, Train Accuracy = 75.49%, Test Accuracy = 73.79%, Epoch Time = 7.70s, Total Time = 123.98s
Epoch 17/50: Train Loss = 0.6810, Train Accuracy = 75.98%, Test Accuracy = 74.26%, Epoch Time = 7.70s, Total Time = 131.68s
Epoch 18/50: Train Loss = 0.6860, Train Accuracy = 76.09%, Test Accuracy = 76.52%, Epoch Time = 7.71s, Total Time = 139.39s
Epoch 19/50: Train Loss = 0.6674, Train Accuracy = 77.06%, Test Accuracy = 73.75%, Epoch Time = 7.64s, Total Time = 147.04s
Epoch 20/50: Train Loss = 0.6609, Train Accuracy = 77.12%, Test Accuracy = 74.10%, Epoch Time = 7.67s, Total Time = 154.71s
Epoch 21/50: Train Loss = 0.5066, Train Accuracy = 83.00%, Test Accuracy = 81.64%, Epoch Time = 7.72s, Total Time = 162.43s
Epoch 22/50: Train Loss = 0.4699, Train Accuracy = 84.25%, Test Accuracy = 82.23%, Epoch Time = 7.74s, Total Time = 170.16s
Epoch 23/50: Train Loss = 0.4388, Train Accuracy = 84.40%, Test Accuracy = 82.58%, Epoch Time = 7.73s, Total Time = 177.90s
Epoch 24/50: Train Loss = 0.4208, Train Accuracy = 85.63%, Test Accuracy = 82.15%, Epoch Time = 7.61s, Total Time = 185.50s
Epoch 25/50: Train Loss = 0.4008, Train Accuracy = 86.06%, Test Accuracy = 83.05%, Epoch Time = 7.68s, Total Time = 193.18s
Epoch 26/50: Train Loss = 0.4020, Train Accuracy = 86.25%, Test Accuracy = 82.46%, Epoch Time = 7.73s, Total Time = 200.91s
Epoch 27/50: Train Loss = 0.3896, Train Accuracy = 86.69%, Test Accuracy = 82.50%, Epoch Time = 7.67s, Total Time = 208.58s
Epoch 28/50: Train Loss = 0.3884, Train Accuracy = 86.27%, Test Accuracy = 83.32%, Epoch Time = 7.68s, Total Time = 216.26s
Epoch 29/50: Train Loss = 0.3708, Train Accuracy = 87.48%, Test Accuracy = 82.77%, Epoch Time = 7.77s, Total Time = 224.03s
Epoch 30/50: Train Loss = 0.3607, Train Accuracy = 87.52%, Test Accuracy = 83.36%, Epoch Time = 7.75s, Total Time = 231.79s
Epoch 31/50: Train Loss = 0.3480, Train Accuracy = 87.83%, Test Accuracy = 83.83%, Epoch Time = 7.88s, Total Time = 239.67s
Epoch 32/50: Train Loss = 0.3371, Train Accuracy = 88.27%, Test Accuracy = 84.06%, Epoch Time = 7.92s, Total Time = 247.59s
Epoch 33/50: Train Loss = 0.3355, Train Accuracy = 88.46%, Test Accuracy = 84.18%, Epoch Time = 7.88s, Total Time = 255.47s
Epoch 34/50: Train Loss = 0.3126, Train Accuracy = 89.05%, Test Accuracy = 84.22%, Epoch Time = 7.95s, Total Time = 263.42s
Epoch 35/50: Train Loss = 0.3166, Train Accuracy = 88.92%, Test Accuracy = 84.02%, Epoch Time = 7.87s, Total Time = 271.29s
Epoch 36/50: Train Loss = 0.3241, Train Accuracy = 88.46%, Test Accuracy = 84.38%, Epoch Time = 7.81s, Total Time = 279.09s
Epoch 37/50: Train Loss = 0.3239, Train Accuracy = 88.88%, Test Accuracy = 84.22%, Epoch Time = 7.73s, Total Time = 286.82s
Epoch 38/50: Train Loss = 0.3223, Train Accuracy = 88.79%, Test Accuracy = 84.61%, Epoch Time = 7.91s, Total Time = 294.73s
Epoch 39/50: Train Loss = 0.3122, Train Accuracy = 89.25%, Test Accuracy = 84.45%, Epoch Time = 7.87s, Total Time = 302.61s
Epoch 40/50: Train Loss = 0.3163, Train Accuracy = 89.09%, Test Accuracy = 84.18%, Epoch Time = 7.86s, Total Time = 310.47s
Epoch 41/50: Train Loss = 0.3016, Train Accuracy = 89.55%, Test Accuracy = 84.26%, Epoch Time = 7.81s, Total Time = 318.28s
Epoch 42/50: Train Loss = 0.3014, Train Accuracy = 89.42%, Test Accuracy = 84.45%, Epoch Time = 7.83s, Total Time = 326.11s
Epoch 43/50: Train Loss = 0.3097, Train Accuracy = 89.21%, Test Accuracy = 84.18%, Epoch Time = 7.64s, Total Time = 333.75s
Epoch 44/50: Train Loss = 0.3163, Train Accuracy = 89.13%, Test Accuracy = 84.53%, Epoch Time = 7.66s, Total Time = 341.41s
Epoch 45/50: Train Loss = 0.3166, Train Accuracy = 88.98%, Test Accuracy = 84.41%, Epoch Time = 7.71s, Total Time = 349.12s
Epoch 46/50: Train Loss = 0.3157, Train Accuracy = 89.20%, Test Accuracy = 84.38%, Epoch Time = 7.75s, Total Time = 356.87s
Epoch 47/50: Train Loss = 0.3085, Train Accuracy = 89.27%, Test Accuracy = 84.22%, Epoch Time = 7.75s, Total Time = 364.62s
Epoch 48/50: Train Loss = 0.3037, Train Accuracy = 89.42%, Test Accuracy = 84.34%, Epoch Time = 7.71s, Total Time = 372.34s
Epoch 49/50: Train Loss = 0.3049, Train Accuracy = 89.31%, Test Accuracy = 84.26%, Epoch Time = 7.67s, Total Time = 380.00s
Epoch 50/50: Train Loss = 0.3021, Train Accuracy = 89.29%, Test Accuracy = 84.22%, Epoch Time = 7.69s, Total Time = 387.69s
Metrics plot saved to training_metrics_with_time.png
Warning: The cache directory for DeepSpeed Triton autotune, /home/hsk8171/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
